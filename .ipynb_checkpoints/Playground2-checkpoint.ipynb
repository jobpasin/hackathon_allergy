{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import unirest\n",
    "import ssl\n",
    "import jaconv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information from image using Microsoft CV API\n",
    "def ocr(url_link):\n",
    "    response = unirest.post(\"https://microsoft-azure-microsoft-computer-vision-v1.p.rapidapi.com/ocr?language=unk\",\n",
    "                            headers={\n",
    "                                \"X-RapidAPI-Key\": \"f61cc459a4msh1088b9535c232b4p1efb59jsn2a694b505fef\",\n",
    "                                \"Content-Type\": \"application/json\"\n",
    "                            },\n",
    "                            params=(\"{\\\"url\\\":\\\"%s\\\"}\" % url_link)\n",
    "                            )\n",
    "    ocr_words = []\n",
    "    result = response.body\n",
    "    # If not detect any text, return 'unk' language\n",
    "    try:\n",
    "        region_word = list()\n",
    "        region_word_bb = list()\n",
    "        language = result[u'language']\n",
    "        for region in result[u'regions']:\n",
    "            for line in region[u'lines']:\n",
    "                sentence = ''\n",
    "                for word in line[u'words']:\n",
    "                    region_word.append(word[u'text'])\n",
    "                    region_word_bb.append(word[u'boundingBox'])\n",
    "                    # print(word[u'text'])\n",
    "                    sentence = sentence + word[u'text']\n",
    "        result_region = {'word': region_word, 'bb': region_word_bb}\n",
    "        ocr_words.append(result_region)\n",
    "        # print(\"Full sentence : \" + word_list_to_str(all_word) + \"\\n\")\n",
    "    except KeyError:\n",
    "        # print(result)\n",
    "        # print(url_link)\n",
    "        all_word = []\n",
    "        all_word_bb = []\n",
    "        language = 'unk'\n",
    "    print(\"Number of regions: %s\" % len(ocr_words))\n",
    "    return ocr_words, language\n",
    "\n",
    "\n",
    "# Check if the \"ingredient\" word exist, if 3 or more exist\n",
    "def check_keyword(keywords, ocr_word):\n",
    "    count = 0\n",
    "    for d in keywords:\n",
    "        for c in ocr_word['word']:\n",
    "            if d == c:\n",
    "                count = count + 1\n",
    "                break\n",
    "    if count >= 2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Keep only dictionary which their values exist in filter_words\n",
    "def filter_values(dict_words, filter_words):\n",
    "    if filter_words is None:\n",
    "        return dict_words\n",
    "    new_dict = {}\n",
    "    for key, values in dict_words.iteritems():\n",
    "        for value in values:\n",
    "            if value in filter_words:\n",
    "                new_dict[key] = values\n",
    "                break\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "# Using freemium microsoft text translation\n",
    "def translate_microsoft(text, language=\"en\"):\n",
    "    url = \"https://microsoft-azure-translation-v1.p.rapidapi.com/translate?to=%s&text=%s\" % (language, text)\n",
    "    url = url.encode('utf-8')\n",
    "    response = unirest.get(url,\n",
    "                           headers={\n",
    "                               \"X-RapidAPI-Key\": \"f61cc459a4msh1088b9535c232b4p1efb59jsn2a694b505fef\"\n",
    "                           }\n",
    "                           )\n",
    "    full_response = response.body\n",
    "    translation = full_response.replace(\"<string xmlns=\\\"http://schemas.microsoft.com/2003/10/Serialization/\\\">\",\n",
    "                                        \"\").replace(\"</string>\", \"\")\n",
    "    return translation\n",
    "\n",
    "\n",
    "# Using Free SYSTRAN.io for optional text translation\n",
    "def translate_sys(text, language=\"en\"):\n",
    "    url = \"https://systran-systran-platform-for-language-processing-v1.p.rapidapi.com/translation/text/translate?source=auto&target=%s&input=%s\" % (\n",
    "        language, text)\n",
    "    url = url.encode('utf-8')\n",
    "    # print(url)\n",
    "    try:\n",
    "        response = unirest.get(url,\n",
    "                               headers={\n",
    "                                   \"X-RapidAPI-Key\": \"f61cc459a4msh1088b9535c232b4p1efb59jsn2a694b505fef\"\n",
    "                               }\n",
    "                               )\n",
    "    except ssl.SSLError:\n",
    "        translation = ['']\n",
    "        print(\"Error at translate_sys. Read operation timed out\")\n",
    "        return translation\n",
    "    try:\n",
    "        translation = response.body['outputs'][0]['output']\n",
    "        translation_conf = response.body['outputs'][0]['detectedLanguageConfidence']\n",
    "    except KeyError:\n",
    "        print(\"Error at translate_sys. Unknown output: %s\" % response.body)\n",
    "        print(\"Input url:\" + url)\n",
    "        translation = ['']\n",
    "    return translation\n",
    "\n",
    "\n",
    "# Return meaning of Japanese word in English, with same bounding box\n",
    "def get_translation_pair(allergy_word_jp, allergy_list_jp_en):\n",
    "    # Get translation pair\n",
    "    allergy_word_eng_word = list()\n",
    "    allergy_word_eng_bb = list()\n",
    "    for i in range(len(allergy_word_jp['word'])):\n",
    "        eng_word = allergy_list_jp_en[allergy_word_jp['word'][i]]\n",
    "        eng_word_bb = allergy_word_jp['bb'][i]\n",
    "        allergy_word_eng_word.append(eng_word)\n",
    "        allergy_word_eng_bb.append(eng_word_bb)\n",
    "    return {'word': allergy_word_eng_word, 'bb': allergy_word_eng_bb}\n",
    "\n",
    "\n",
    "# Given list of string and a string, return list of index where \"word\" exist\n",
    "def get_index(list_of_word, word):\n",
    "    index = list()\n",
    "    for i, j in enumerate(list_of_word):\n",
    "        if j == word:\n",
    "            index.append(i)\n",
    "    return index\n",
    "\n",
    "\n",
    "# Given a list of string, return only elements which exist in text\n",
    "def check_exist(word_list, text):\n",
    "    exist_word = list()\n",
    "    for word in word_list:\n",
    "        if word in text:\n",
    "            exist_word.append(word)\n",
    "    return exist_word\n",
    "\n",
    "\n",
    "# Get values (english word) from allergy_pair\n",
    "def get_english_allergy_list(allergy_pair):\n",
    "    english_allergy_list = list()\n",
    "    for value in allergy_pair.values():\n",
    "        for w in value:\n",
    "            english_allergy_list.append(w)\n",
    "    english_allergy_list = list(set(english_allergy_list))\n",
    "    return english_allergy_list\n",
    "\n",
    "\n",
    "# Convert list of japanese characters into full string, can have delimiter between elements\n",
    "def word_list_to_str(word_list, delimiter=''):\n",
    "    word = ''\n",
    "    for count, letter in enumerate(word_list):\n",
    "        if count != len(word_list) - 1:\n",
    "            word = word + letter + delimiter\n",
    "        else:\n",
    "            word = word + letter\n",
    "    return word\n",
    "\n",
    "\n",
    "# Check all elements in all_word, return all element which exist in allergy list\n",
    "def check_allergy(all_word, aller_list):\n",
    "    allergy_word = list()\n",
    "    allergy_word_bb = list()\n",
    "    for allergy in aller_list:  # allergy = '海老'\n",
    "        matched_word_index = get_index(all_word['word'], allergy[0])  # position of word of 海 e.g. [5, 11]\n",
    "        if len(matched_word_index) > 0:  # Check if there's word exist\n",
    "            for checked_position in matched_word_index:  # checked_position = 5\n",
    "                combined_word = ''\n",
    "                combined_word_bb = list()\n",
    "                for cnt in range(checked_position, checked_position + len(allergy)):  # cnt = 5,6\n",
    "                    if cnt >= len(all_word['word']):  # stop if it's over the length of the word\n",
    "                        break\n",
    "                    combined_word = combined_word + all_word['word'][cnt]  # Combine words from the interested point\n",
    "                    combined_word_bb.append(all_word['bb'][cnt])\n",
    "                if combined_word == allergy:  # Compare combined word with the allergy, include if equal\n",
    "                    allergy_word.append(combined_word)\n",
    "                    allergy_word_bb.append(combined_word_bb)\n",
    "    return {'word': allergy_word, 'bb': allergy_word_bb}\n",
    "\n",
    "\n",
    "# Check elements that co-exist between two lists\n",
    "def cross_check(ocr_list, trans_list_one, trans_list_two):\n",
    "    # Flatten ocr_list and Remove duplicates\n",
    "    ocr_list_temp = [item for sublist in ocr_list for item in sublist]\n",
    "    ocr_list = list(set(ocr_list_temp))\n",
    "    word_list_one = list(set(trans_list_one))\n",
    "    word_list_two = list(set(trans_list_two))\n",
    "\n",
    "    result = {'exist_3': list(), 'exist_2': list(), 'exist_1': list()}\n",
    "\n",
    "    for word in ocr_list:\n",
    "        count = 1\n",
    "        if word in word_list_one:\n",
    "            count = count + 1\n",
    "            word_list_one.remove(word)\n",
    "        if word in word_list_two:\n",
    "            count = count + 1\n",
    "            word_list_two.remove(word)\n",
    "        name = 'exist_' + str(count)\n",
    "        result[name].append(word)\n",
    "    for word in word_list_one:\n",
    "        count = 1\n",
    "        if word in word_list_two:\n",
    "            count = count + 1\n",
    "            word_list_two.remove(word)\n",
    "        name = 'exist_' + str(count)\n",
    "        result[name].append(word)\n",
    "    for word in word_list_two:\n",
    "        count = 1\n",
    "        name = 'exist_' + str(count)\n",
    "        result[name].append(word)\n",
    "    return {'result': result}\n",
    "\n",
    "\n",
    "# Given two dictionaries with same keys, combined together\n",
    "def merge_result(result_main, result):\n",
    "    result_new_temp = {'exist_3': result_main['exist_3'] + list(set(result['exist_3']) - set(result_main['exist_3'])),\n",
    "                       'exist_2': result_main['exist_2'] + list(set(result['exist_2']) - set(result_main['exist_2'])),\n",
    "                       'exist_1': result_main['exist_1'] + list(set(result['exist_1']) - set(result_main['exist_1']))}\n",
    "\n",
    "    result_new = {'exist_3': result_new_temp['exist_3'],\n",
    "                  'exist_2': list(set(result_new_temp['exist_2']) - set(result_new_temp['exist_3'])),\n",
    "                  'exist_1': list(set(result_new_temp['exist_1']) - set(result_new_temp['exist_2']) - set(\n",
    "                      result_new_temp['exist_3']))}\n",
    "    return result_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Function for us\n",
    "def translate_jp_region(ocr_word, allergies):\n",
    "    ingredient_char = [u'原', u'材', u'料', u'名']\n",
    "    # List of allergies with their meaning\n",
    "    standard_allergy_pair = {u'海老': ['shrimp', 'prawn'],\n",
    "                             u'エビ': ['shrimp', 'prawn'],\n",
    "                             u'えび': ['shrimp', 'prawn'],\n",
    "                             u'かに': ['crab'],\n",
    "                             u'小麦': ['wheat', 'flour'],\n",
    "                             u'そば': ['buckwheat', 'soba'],\n",
    "                             u'卵': ['egg'],\n",
    "                             u'たまご': ['egg'],\n",
    "                             u'乳': ['milk'],\n",
    "                             u'落花生': ['peanut'],\n",
    "                             u'あわび': ['abalone'],\n",
    "                             u'いか': ['squid'],\n",
    "                             u'いくら': ['salmon roe'],\n",
    "                             u'オレンジ': ['orange'],\n",
    "                             u'カシューナッツ': ['cashew nut'],\n",
    "                             u'キウイフルーツ': ['kiwifruit'],\n",
    "                             u'牛肉': ['beef'],\n",
    "                             u'くるみ': ['walnuts'],\n",
    "                             u'ごま': ['sesame'],\n",
    "                             u'鮭': ['salmon'],\n",
    "                             u'さけ': ['salmon'],\n",
    "                             u'さば': ['mackerel'],\n",
    "                             u'大豆': ['soybeans'],\n",
    "                             u'鶏肉': ['chicken'],\n",
    "                             u'バナナ': ['banana'],\n",
    "                             u'豚肉': ['pork'],\n",
    "                             u'まつたけ': ['matsutake'],\n",
    "                             u'もも': ['peaches'],\n",
    "                             u'やまいも': ['yams'],\n",
    "                             u'りんご': ['apple'],\n",
    "                             u'ゼラチン': ['gelatin']}\n",
    "\n",
    "    # Get only interested allergy words (from user)\n",
    "    if allergies is None:\n",
    "        allergy_pair_jp_en = standard_allergy_pair\n",
    "    elif len(allergies) == 0:\n",
    "        allergy_pair_jp_en = standard_allergy_pair\n",
    "    else:\n",
    "        allergy_pair_jp_en = filter_values(standard_allergy_pair, allergies)\n",
    "    allergy_list_eng = get_english_allergy_list(allergy_pair_jp_en)\n",
    "\n",
    "    # Check if there is word \"ingredients inside\", return boolean (Should be True)\n",
    "    keyword_exist = check_keyword(ingredient_char, ocr_word)\n",
    "    if not keyword_exist:\n",
    "        result = {\"error\": \"Ingredients not found\"}\n",
    "    else:\n",
    "        # Compare ocr result with allergy list, output is dictionary\n",
    "        ocr_allergy_jp = check_allergy(ocr_word, allergy_pair_jp_en.keys())\n",
    "        # Get translation of allergies in English, output is dictionary\n",
    "        ocr_allergy_en = get_translation_pair(ocr_allergy_jp, allergy_pair_jp_en)\n",
    "        # Translate detected word into English\n",
    "        translation_micr = translate_microsoft(word_list_to_str(ocr_word['word']))  # Takes 1.6 sec\n",
    "        translation_syst = translate_sys(word_list_to_str(ocr_word['word']))  # Takes 1.85 sec\n",
    "        # Check if allergy word exist in the translation or not\n",
    "        translate_allergy_micr = check_exist(allergy_list_eng, translation_micr)\n",
    "        translate_allergy_syst = check_exist(allergy_list_eng, translation_syst)\n",
    "        # Check between three methods, give confidence level\n",
    "        result = cross_check(ocr_allergy_en['word'], translate_allergy_micr, translate_allergy_syst)\n",
    "        # Print allergies word\n",
    "        # print(\"OCR Allergies word(JP) result: \" + word_list_to_str(ocr_allergy_jp['word'], ','))\n",
    "        # print(\"OCR Allergies word(EN) result: \" + str(ocr_allergy_en['word']))\n",
    "        # print(\"Translated allergies word(Microsoft) result: \" + str(translate_allergy_micr))\n",
    "        # print(\"Translated allergies word(Systran) result: \" + str(translate_allergy_syst))\n",
    "    return result\n",
    "\n",
    "\n",
    "def translate_en_region(ocr_word):  # Not using right now\n",
    "    allergy_list_eng = ['shrimp', 'crab', 'wheat', 'soba', 'egg', 'milk', 'peanut']  # Temp\n",
    "    ocr_allergy = check_exist(ocr_word['word'], allergy_list_eng)  # Note that we lose bounding box info here\n",
    "    result = {\"result\": ocr_allergy}\n",
    "    return result\n",
    "\n",
    "\n",
    "# For other language, we use APIs to translate english word to another language and cross-check with OCR\n",
    "def translate_other_region(ocr_word, allergy_list_en, language):\n",
    "    # keyword = \"name of ingredient\"\n",
    "    # Get list of allergies in desired language\n",
    "    if allergy_list_en is None:\n",
    "        allergy_list_en = ['shrimp', 'crab', 'wheat', 'soba', 'egg', 'milk', 'peanut']\n",
    "    elif len(allergy_list_en) == 0:\n",
    "        allergy_list_en = ['shrimp', 'crab', 'wheat', 'soba', 'egg', 'milk', 'peanut']\n",
    "    allergy_string_en = word_list_to_str(allergy_list_en, delimiter='_')\n",
    "    allergy_string_other = translate_sys(allergy_string_en, language)\n",
    "    allergy_list_other = allergy_string_other.split('_')\n",
    "    # print(\"Detect other language: %s\" % language)\n",
    "    if len(allergy_list_other) != len(allergy_list_en):\n",
    "        raise ValueError(\"Translation conflited. Input as language: %s, Message: %s\" % (language, ocr_word))\n",
    "    # Create allergy pair as same as translate_jp\n",
    "    allergy_pair = {}\n",
    "    for i, value in enumerate(allergy_list_en):\n",
    "        allergy_pair[allergy_list_other[i]] = value\n",
    "\n",
    "    # Compare ocr result with allergy list, output is dictionary\n",
    "    ocr_allergy_other = check_allergy(ocr_word, allergy_list_other)\n",
    "    # Get translation of allergies in English, output is dictionary\n",
    "    ocr_allergy_en = get_translation_pair(ocr_allergy_other, allergy_pair)\n",
    "    # Translate detected word into English\n",
    "    translation_micr = translate_microsoft(word_list_to_str(ocr_word['word']))  # Takes 1.6 sec\n",
    "    translation_syst = translate_sys(word_list_to_str(ocr_word['word']))  # Takes 1.85 sec\n",
    "    # Check if allergy word exist in the translation or not\n",
    "    translate_allergy_micr = check_exist(allergy_list_en, translation_micr)\n",
    "    translate_allergy_syst = check_exist(allergy_list_en, translation_syst)\n",
    "    result = cross_check(ocr_allergy_en['word'], translate_allergy_micr, translate_allergy_syst)\n",
    "    # Print allergies word\n",
    "    # print(\"OCR Allergies word(%s) result: \" % language) result: \" + word_list_to_str(ocr_allergy_other['word'], ','))\n",
    "    # print((\"OCR Allergies word(EN) + str(ocr_allergy_en['word']))\n",
    "    # print(\"Translated allergies word(Microsoft) result: \" + str(translate_allergy_micr))\n",
    "    # print(\"Translated allergies word(Systran) result: \" + str(translate_allergy_syst))\n",
    "    return result\n",
    "\n",
    "\n",
    "# Based on ocr, we split set of text into regions, we translate one region and a time\n",
    "def translate_jp(ocr_words, allergies):\n",
    "    total_result = {'exist_3': [], 'exist_2': [], 'exist_1': []}\n",
    "    count = 0\n",
    "    for i in range(len(ocr_words)):\n",
    "        region_result = translate_jp_region(ocr_words[i], allergies)\n",
    "        if 'error' in region_result:  # If not found keyword, skip doing the rest\n",
    "            count = count + 1\n",
    "            break\n",
    "        # Combined result from the current one with the previous one\n",
    "        total_result = merge_result(total_result, region_result['result'])\n",
    "    if (count == len(ocr_words)) and (count != 0):\n",
    "        total_result = {\"error\": \"Ingredients not found\"}\n",
    "    return {'result': total_result}\n",
    "\n",
    "\n",
    "def translate_en(ocr_words):  # Unused Function\n",
    "    result = list()\n",
    "    for i in range(len(ocr_words)):\n",
    "        region_result = translate_en_region(ocr_words[i])\n",
    "        result = result + list(set(region_result) - set(result))\n",
    "    return {'result': result}\n",
    "\n",
    "\n",
    "def translate_other(ocr_words, allergies, language):\n",
    "    total_result = {'exist_3': [], 'exist_2': [], 'exist_1': []}\n",
    "    count = 0\n",
    "    for i in range(len(ocr_words)):\n",
    "        region_result = translate_other_region(ocr_words[i], allergies, language)\n",
    "        if 'error' in region_result:  # If not found keyword, skip doing the rest\n",
    "            count = count + 1\n",
    "            break\n",
    "        # Combined result from the current one with the previous one\n",
    "        total_result = merge_result(total_result, region_result['result'])\n",
    "    if (count == len(ocr_words)) and (len(ocr_words) != 0):\n",
    "        total_result = {\"error\": \"Cannot translate allergies name. This language is not supported (%s)\" % language}\n",
    "    return {'result': total_result}\n",
    "\n",
    "\n",
    "# Determine function based on language detected\n",
    "def get_allergies(url_link, allergies=None):\n",
    "    ocr_word, language = ocr(url_link)  # Take 2.67 secx\n",
    "    if language == 'unk':\n",
    "        result = {'error': \"No text detected\"}\n",
    "    elif language == 'ja':  # For default\n",
    "        result = translate_jp(ocr_word, allergies)\n",
    "    elif language == 'en':  # Not supported yet\n",
    "        result = {'error': \"No japanese detected\"}\n",
    "    else:  # For other language\n",
    "        result = translate_other(ocr_word, allergies, language)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_link = \"https://res.cloudinary.com/pasink/image/upload/v1550237262/test_hd.jpg\"  # First Sample\n",
    "# url_link = \"https://res.cloudinary.com/pasink/image/upload/v1550300203/test2.jpg\"  # Second Sample, MS error\n",
    "# url_link = \"https://res.cloudinary.com/pasink/image/upload/v1550289399/vertical1.jpg\" # Not found Sample\n",
    "# url_link = \"https://res.cloudinary.com/pasink/image/upload/v1550278845/korean.jpg\"  # Korean file\n",
    "url_link = \"https://allergynode.herokuapp.com/photos/test-photo.jpg\"\n",
    "\n",
    "final_result = get_allergies(url_link=url_link, allergies=None)\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
